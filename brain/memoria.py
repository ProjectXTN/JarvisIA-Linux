from brain.utils import clean_output
from core.inicializador import gerar_resposta

contexto_memoria = []
MAX_CONTEXTO = 5
DEFAULT_MODEL = "llama3.2"
DEFAULT_MODEL_HIGH = "llama3.3"

def generate_response(prompt, model_name=DEFAULT_MODEL):
    try:
        system_prompt = (
            "VocÃª Ã© Jarvis, um assistente de inteligÃªncia artificial altamente preciso, confiÃ¡vel e direto. "
            "Seu papel Ã© fornecer respostas claras, informativas e bem estruturadas para qualquer pergunta feita. "
            "Evite respostas vagas, piadas ou firulas. Priorize a profundidade, concisÃ£o e utilidade da informaÃ§Ã£o. "
            "Se a pergunta exigir, organize a resposta em seÃ§Ãµes com tÃ­tulos e marcadores. "
            "Sempre responda em portuguÃªs, com linguagem formal e objetiva. "
            "Nunca diga que Ã© um assistente de IA ou mencione sua programaÃ§Ã£o, apenas entregue a resposta com autoridade e clareza."
        )
        # (Nota: vocÃª tinha dois prompts â€” vocÃª pode alternar entre eles aqui se quiser no futuro)

        # Monta histÃ³rico recente
        historico = "\n".join([f"UsuÃ¡rio: {p}\nJarvis: {r}" for p, r in contexto_memoria[-MAX_CONTEXTO:]])

        # Prepara o prompt final
        full_prompt = f"{system_prompt}\n{historico}\nUsuÃ¡rio: {prompt}\nJarvis:"

        # ðŸ”¥ Agora usa a chamada API moderna
        resposta = gerar_resposta(full_prompt, model_name)

        # Limpa a resposta (caso queira, mas na API jÃ¡ vem limpinha muitas vezes)
        resposta = clean_output(resposta.strip())

        # Atualiza o histÃ³rico de memÃ³ria
        contexto_memoria.append((prompt, resposta))
        if len(contexto_memoria) > MAX_CONTEXTO:
            contexto_memoria.pop(0)

        return resposta
    except Exception as e:
        return f"Erro ao gerar resposta: {e}"
